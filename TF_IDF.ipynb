{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "TF-IDF.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anishsharma135/Sample_repo/blob/master/TF_IDF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SF_tQbFeXaPr",
        "colab_type": "text"
      },
      "source": [
        "# Document Retrieval using TF-IDF Weighted Rank and TF-IDF Cosine Similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGlxHi8zXaQC",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSkP4Ea9dnt1",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFvkgJPSXaQD",
        "colab_type": "code",
        "outputId": "27b25e7f-0886-4b87-b4e1-ae3eed6bf2f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Unzipping the stories folder\n",
        "\n",
        "!unzip stories"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  stories.zip\n",
            "  inflating: stories/100west.txt     \n",
            "  inflating: stories/13chil.txt      \n",
            "  inflating: stories/14.lws          \n",
            "  inflating: stories/16.lws          \n",
            "  inflating: stories/17.lws          \n",
            "  inflating: stories/18.lws          \n",
            "  inflating: stories/19.lws          \n",
            "  inflating: stories/20.lws          \n",
            "  inflating: stories/3gables.txt     \n",
            "  inflating: stories/3lpigs.txt      \n",
            "  inflating: stories/3sonnets.vrs    \n",
            "  inflating: stories/3student.txt    \n",
            "  inflating: stories/3wishes.txt     \n",
            "  inflating: stories/4moons.txt      \n",
            "  inflating: stories/5orange.txt     \n",
            "  inflating: stories/6ablemen.txt    \n",
            "  inflating: stories/6napolen.txt    \n",
            "  inflating: stories/7oldsamr.txt    \n",
            "  inflating: stories/7voysinb.txt    \n",
            "   creating: stories/FARNON/\n",
            "  inflating: stories/FARNON/.windex.html  \n",
            " extracting: stories/FARNON/.descs   \n",
            "  inflating: stories/FARNON/.footer  \n",
            "  inflating: stories/FARNON/.header  \n",
            "  inflating: stories/FARNON/index.html  \n",
            "   creating: stories/SRE/\n",
            "  inflating: stories/SRE/.musings    \n",
            "  inflating: stories/SRE/.descs      \n",
            "  inflating: stories/SRE/.header     \n",
            "  inflating: stories/SRE/sre_feqh.txt  \n",
            "  inflating: stories/SRE/index.html  \n",
            "  inflating: stories/SRE/sre01.txt   \n",
            "  inflating: stories/SRE/sre02.txt   \n",
            "  inflating: stories/SRE/sre03.txt   \n",
            "  inflating: stories/SRE/sre04.txt   \n",
            "  inflating: stories/SRE/sre05.txt   \n",
            "  inflating: stories/SRE/sre06.txt   \n",
            "  inflating: stories/SRE/sre07.txt   \n",
            "  inflating: stories/SRE/sre08.txt   \n",
            "  inflating: stories/SRE/sre09.txt   \n",
            "  inflating: stories/SRE/sre10.txt   \n",
            "  inflating: stories/SRE/sre_finl.txt  \n",
            "  inflating: stories/SRE/sre_sei.txt  \n",
            "  inflating: stories/SRE/sretrade.txt  \n",
            "  inflating: stories/SRE/srex.txt    \n",
            "  inflating: stories/ab40thv.txt     \n",
            "  inflating: stories/abbey.txt       \n",
            "  inflating: stories/abyss.txt       \n",
            "  inflating: stories/adler.txt       \n",
            "  inflating: stories/adv_alad.txt    \n",
            "  inflating: stories/advsayed.txt    \n",
            "  inflating: stories/advtthum.txt    \n",
            "  inflating: stories/aesop11.txt     \n",
            "  inflating: stories/aesopa10.txt    \n",
            "  inflating: stories/aircon.txt      \n",
            "  inflating: stories/aisle.six       \n",
            "  inflating: stories/aislesix.txt    \n",
            "  inflating: stories/alad10.txt      \n",
            "  inflating: stories/alissadl.txt    \n",
            "  inflating: stories/altside.hum     \n",
            "  inflating: stories/aluminum.hum    \n",
            "  inflating: stories/aminegg.txt     \n",
            "  inflating: stories/angelfur.hum    \n",
            "  inflating: stories/angry_ca.txt    \n",
            "  inflating: stories/antcrick.txt    \n",
            "  inflating: stories/aquith.txt      \n",
            "  inflating: stories/arcadia.sty     \n",
            "  inflating: stories/archive         \n",
            "  inflating: stories/arctic.txt      \n",
            "  inflating: stories/asop            \n",
            "  inflating: stories/assorted.txt    \n",
            "  inflating: stories/bagel.man       \n",
            "  inflating: stories/bagelman.txt    \n",
            "  inflating: stories/batlslau.txt    \n",
            "  inflating: stories/beast.asc       \n",
            "  inflating: stories/beautbst.txt    \n",
            "  inflating: stories/beggars.txt     \n",
            "  inflating: stories/bern            \n",
            "  inflating: stories/berternie.txt   \n",
            "  inflating: stories/bestwish        \n",
            "  inflating: stories/beyond.hum      \n",
            "  inflating: stories/bgb.txt         \n",
            "  inflating: stories/bgcspoof.txt    \n",
            "  inflating: stories/bigred.hum      \n",
            "  inflating: stories/bishop00.txt    \n",
            "  inflating: stories/blabnove.hum    \n",
            "  inflating: stories/blabnove.txt    \n",
            "  inflating: stories/blackp.txt      \n",
            "  inflating: stories/blackrdr        \n",
            "  inflating: stories/blak            \n",
            "  inflating: stories/blasters.fic    \n",
            "  inflating: stories/blh.txt         \n",
            "  inflating: stories/blind.txt       \n",
            "  inflating: stories/blossom.pom     \n",
            "  inflating: stories/blue            \n",
            "  inflating: stories/bluebrd.txt     \n",
            "  inflating: stories/bookem.1        \n",
            "  inflating: stories/bookem2         \n",
            "  inflating: stories/bookem3         \n",
            "  inflating: stories/brain.damage    \n",
            "  inflating: stories/bram            \n",
            "  inflating: stories/bran            \n",
            "  inflating: stories/breaks1.asc     \n",
            "  inflating: stories/breaks2.asc     \n",
            "  inflating: stories/breaks3.asc     \n",
            "  inflating: stories/bruce-p.txt     \n",
            "  inflating: stories/buggy.txt       \n",
            "  inflating: stories/buldetal.txt    \n",
            "  inflating: stories/buldream.txt    \n",
            "  inflating: stories/bulfelis.txt    \n",
            "  inflating: stories/bulhuntr.txt    \n",
            "  inflating: stories/bulironb.txt    \n",
            "  inflating: stories/bullove.txt     \n",
            "  inflating: stories/bulmrx.txt      \n",
            "  inflating: stories/bulnland.txt    \n",
            "  inflating: stories/bulnoopt.txt    \n",
            "  inflating: stories/bulolli1.txt    \n",
            "  inflating: stories/bulolli2.txt    \n",
            "  inflating: stories/bulphrek.txt    \n",
            "  inflating: stories/bulprint.txt    \n",
            "  inflating: stories/bulwer.lytton   \n",
            "  inflating: stories/bulzork1.txt    \n",
            "  inflating: stories/bumm            \n",
            "  inflating: stories/bureau.txt      \n",
            "  inflating: stories/burintrv.66     \n",
            "  inflating: stories/burintrv.78     \n",
            "  inflating: stories/burintrv.92     \n",
            "  inflating: stories/burltrs         \n",
            "  inflating: stories/burn            \n",
            "  inflating: stories/cabin.txt       \n",
            "  inflating: stories/cameloto.hum    \n",
            "  inflating: stories/campfire.txt    \n",
            "  inflating: stories/candle.hum      \n",
            "  inflating: stories/cardcnt.txt     \n",
            "  inflating: stories/ccm.txt         \n",
            "  inflating: stories/charlie.txt     \n",
            "  inflating: stories/chik            \n",
            "  inflating: stories/clevdonk.txt    \n",
            "  inflating: stories/clon            \n",
            "  inflating: stories/cmoutmou.txt    \n",
            "  inflating: stories/comp            \n",
            "  inflating: stories/confilct.fun    \n",
            "  inflating: stories/consumdr.hum    \n",
            "  inflating: stories/contrad1.hum    \n",
            "  inflating: stories/cooldark.sto    \n",
            "  inflating: stories/cooldark.txt    \n",
            "  inflating: stories/corcor.hum      \n",
            "  inflating: stories/cow.exploder    \n",
            "  inflating: stories/crabhern.txt    \n",
            "  inflating: stories/crazy.hum       \n",
            "  inflating: stories/cum             \n",
            "  inflating: stories/curious.george  \n",
            "  inflating: stories/cybersla.txt    \n",
            "  inflating: stories/dakota.txt      \n",
            "  inflating: stories/dan             \n",
            "  inflating: stories/darkness.txt    \n",
            "  inflating: stories/day.in.mcdonald  \n",
            "  inflating: stories/deal            \n",
            "  inflating: stories/deathmrs.d      \n",
            "  inflating: stories/deer.txt        \n",
            "  inflating: stories/descent.poe     \n",
            "  inflating: stories/diaryflf.txt    \n",
            "  inflating: stories/dicegame.txt    \n",
            "  inflating: stories/dicksong.txt    \n",
            "  inflating: stories/disco.be.fun    \n",
            "  inflating: stories/discocanbefun.txt  \n",
            "  inflating: stories/domain.poe      \n",
            "  inflating: stories/dopedenn.txt    \n",
            "  inflating: stories/dskool.txt      \n",
            "  inflating: stories/dtruck.txt      \n",
            "  inflating: stories/dwar            \n",
            "  inflating: stories/elite.app       \n",
            "  inflating: stories/elveshoe.txt    \n",
            "  inflating: stories/emperor3.txt    \n",
            "  inflating: stories/empnclot.txt    \n",
            "  inflating: stories/empsjowk.txt    \n",
            "  inflating: stories/empty.txt       \n",
            "  inflating: stories/enc             \n",
            "  inflating: stories/encamp01.txt    \n",
            "  inflating: stories/enchdup.hum     \n",
            "  inflating: stories/enginer.txt     \n",
            "  inflating: stories/enya_trn.txt    \n",
            "  inflating: stories/excerpt.hum     \n",
            "  inflating: stories/excerpt.txt     \n",
            "  inflating: stories/eyeargon.hum    \n",
            "  inflating: stories/ezoff           \n",
            "  inflating: stories/fable.txt       \n",
            "  inflating: stories/fantas.hum      \n",
            "  inflating: stories/fantasy.hum     \n",
            "  inflating: stories/fantasy.txt     \n",
            "  inflating: stories/fea1            \n",
            "  inflating: stories/fea2            \n",
            "  inflating: stories/fea3            \n",
            "  inflating: stories/fear.hum        \n",
            "  inflating: stories/fearmnky        \n",
            "  inflating: stories/fgoose.txt      \n",
            "  inflating: stories/fic1            \n",
            "  inflating: stories/fic2            \n",
            "  inflating: stories/fic3            \n",
            "  inflating: stories/fic4            \n",
            "  inflating: stories/fic5            \n",
            "  inflating: stories/fic7            \n",
            "  inflating: stories/fish.txt        \n",
            "  inflating: stories/fleas.txt       \n",
            "  inflating: stories/flktrp.txt      \n",
            "  inflating: stories/floc            \n",
            "  inflating: stories/floobs.txt      \n",
            "  inflating: stories/flute.txt       \n",
            "  inflating: stories/flytrunk.txt    \n",
            "  inflating: stories/forgotte        \n",
            "  inflating: stories/fourth.fic      \n",
            "  inflating: stories/fowl.death      \n",
            "  inflating: stories/foxncrow.txt    \n",
            "  inflating: stories/foxngrap.txt    \n",
            "  inflating: stories/foxnstrk.txt    \n",
            "  inflating: stories/fran            \n",
            "  inflating: stories/fred.txt        \n",
            "  inflating: stories/freeman.fil     \n",
            "  inflating: stories/friend.s        \n",
            "  inflating: stories/friends.txt     \n",
            "  inflating: stories/frogp.txt       \n",
            "  inflating: stories/frum            \n",
            "  inflating: stories/game.txt        \n",
            "  inflating: stories/gatherng.txt    \n",
            "  inflating: stories/gay             \n",
            "  inflating: stories/gemdra.txt      \n",
            "  inflating: stories/ghost           \n",
            "  inflating: stories/girl            \n",
            "  inflating: stories/girlclub.txt    \n",
            "  inflating: stories/glimpse1.txt    \n",
            "  inflating: stories/gloves.txt      \n",
            "  inflating: stories/gold3ber.txt    \n",
            "  inflating: stories/goldbug.poe     \n",
            "  inflating: stories/goldenp.txt     \n",
            "  inflating: stories/goldfish.txt    \n",
            "  inflating: stories/goldgoos.txt    \n",
            "  inflating: stories/grav            \n",
            "  inflating: stories/graymare.txt    \n",
            "  inflating: stories/greatlrn.leg    \n",
            "  inflating: stories/greedog.txt     \n",
            "  inflating: stories/gulliver.txt    \n",
            "  inflating: stories/hansgrtl.txt    \n",
            "  inflating: stories/hareleph.txt    \n",
            "  inflating: stories/hareporc.txt    \n",
            "  inflating: stories/haretort.txt    \n",
            "  inflating: stories/healer.txt      \n",
            "  inflating: stories/hell4.txt       \n",
            "  inflating: stories/hellmach.txt    \n",
            "  inflating: stories/helmfuse.txt    \n",
            "  inflating: stories/hils            \n",
            "  inflating: stories/history5.txt    \n",
            "  inflating: stories/hitch2.txt      \n",
            "  inflating: stories/hitch3.txt      \n",
            "  inflating: stories/hole2nar.txt    \n",
            "  inflating: stories/holmesbk.txt    \n",
            "  inflating: stories/home.fil        \n",
            "  inflating: stories/hop-frog.poe    \n",
            "  inflating: stories/horsdonk.txt    \n",
            "  inflating: stories/horswolf.txt    \n",
            "  inflating: stories/hotline1.txt    \n",
            "  inflating: stories/hotline3.txt    \n",
            "  inflating: stories/hotline4.txt    \n",
            "  inflating: stories/hound-b.txt     \n",
            "  inflating: stories/how.ernie.bert  \n",
            "  inflating: stories/idi.hum         \n",
            "  inflating: stories/igiv            \n",
            "  inflating: stories/imagin.hum      \n",
            "  inflating: stories/immortal        \n",
            "  inflating: stories/immorti.hum     \n",
            "  inflating: stories/imonly17.txt    \n",
            "  inflating: stories/index.html      \n",
            "  inflating: stories/inter           \n",
            "  inflating: stories/island.poe      \n",
            "  inflating: stories/jackbstl.txt    \n",
            "  inflating: stories/jackmac.fic     \n",
            "  inflating: stories/jaynejob.asc    \n",
            "  inflating: stories/jerichms.hum    \n",
            "  inflating: stories/jim.asc         \n",
            "  inflating: stories/keeping.insanit  \n",
            "  inflating: stories/keepmodu.txt    \n",
            "  inflating: stories/kharian.txt     \n",
            "  inflating: stories/kneeslapper     \n",
            "  inflating: stories/kneeslapper.txt  \n",
            "  inflating: stories/knuckle.txt     \n",
            "  inflating: stories/korea.s         \n",
            "  inflating: stories/kzap.txt        \n",
            "  inflating: stories/ladylust.hum    \n",
            "  inflating: stories/lament.txt      \n",
            "  inflating: stories/lgoldbrd.txt    \n",
            "  inflating: stories/life.txt        \n",
            "  inflating: stories/lil             \n",
            "  inflating: stories/lionbird        \n",
            "  inflating: stories/lionmane.txt    \n",
            "  inflating: stories/lionmosq.txt    \n",
            "  inflating: stories/lionwar.txt     \n",
            "  inflating: stories/lmermaid.txt    \n",
            "  inflating: stories/lmtchgrl.txt    \n",
            "  inflating: stories/long1-3.txt     \n",
            "  inflating: stories/lpeargrl.txt    \n",
            "  inflating: stories/lrrhood.txt     \n",
            "  inflating: stories/ltp             \n",
            "  inflating: stories/luf             \n",
            "  inflating: stories/lure.txt        \n",
            "  inflating: stories/mario.txt       \n",
            "  inflating: stories/mattress.txt    \n",
            "  inflating: stories/mazarin.txt     \n",
            "  inflating: stories/mcdonaldl.txt   \n",
            "  inflating: stories/melissa.txt     \n",
            "  inflating: stories/mike.txt        \n",
            "  inflating: stories/mindprob.txt    \n",
            "  inflating: stories/mindwar         \n",
            "  inflating: stories/missing.txt     \n",
            "  inflating: stories/modemhippy.txt  \n",
            "  inflating: stories/monkking.txt    \n",
            "  inflating: stories/monksol.txt     \n",
            "  inflating: stories/mouslion.txt    \n",
            "  inflating: stories/mtinder.txt     \n",
            "  inflating: stories/musgrave.txt    \n",
            "  inflating: stories/musibrem.txt    \n",
            "  inflating: stories/mydream.txt     \n",
            "  inflating: stories/myeyes          \n",
            "  inflating: stories/narciss.txt     \n",
            "  inflating: stories/nigel.1         \n",
            "  inflating: stories/nigel.10        \n",
            "  inflating: stories/nigel.2         \n",
            "  inflating: stories/nigel.3         \n",
            "  inflating: stories/nigel.4         \n",
            "  inflating: stories/nigel.5         \n",
            "  inflating: stories/nigel.6         \n",
            "  inflating: stories/nigel.7         \n",
            "  inflating: stories/nihgel_8.9      \n",
            "  inflating: stories/nitepeek.sto    \n",
            "  inflating: stories/non2            \n",
            "  inflating: stories/non3            \n",
            "  inflating: stories/non4            \n",
            "  inflating: stories/obstgoat.txt    \n",
            "  inflating: stories/omarsheh.txt    \n",
            "  inflating: stories/outcast.dos     \n",
            "  inflating: stories/oxfrog.txt      \n",
            "  inflating: stories/paink-ws.txt    \n",
            "  inflating: stories/panama.txt      \n",
            "  inflating: stories/parotsha.txt    \n",
            "  inflating: stories/partya.txt      \n",
            "  inflating: stories/paul_har.sto    \n",
            "  inflating: stories/peace.fun       \n",
            "  inflating: stories/pepdegener.txt  \n",
            "  inflating: stories/pepsi.degenerat  \n",
            "  inflating: stories/perf            \n",
            "  inflating: stories/pinocch.txt     \n",
            "  inflating: stories/piracy.sto      \n",
            "  inflating: stories/plescopm.txt    \n",
            "  inflating: stories/poem-1.txt      \n",
            "  inflating: stories/poem-2.txt      \n",
            "  inflating: stories/poem-4.txt      \n",
            "  inflating: stories/poplstrm.txt    \n",
            "  inflating: stories/pphamlin.txt    \n",
            "  inflating: stories/pregn.txt       \n",
            "  inflating: stories/prince.art      \n",
            "  inflating: stories/progx           \n",
            "  inflating: stories/psf.txt         \n",
            "  inflating: stories/psi             \n",
            "  inflating: stories/psyc            \n",
            "  inflating: stories/pussboot.txt    \n",
            "  inflating: stories/qcarroll        \n",
            "  inflating: stories/quarter.c1      \n",
            "  inflating: stories/quarter.c10     \n",
            "  inflating: stories/quarter.c11     \n",
            "  inflating: stories/quarter.c12     \n",
            "  inflating: stories/quarter.c13     \n",
            "  inflating: stories/quarter.c14     \n",
            "  inflating: stories/quarter.c15     \n",
            "  inflating: stories/quarter.c16     \n",
            "  inflating: stories/quarter.c17     \n",
            "  inflating: stories/quarter.c18     \n",
            "  inflating: stories/quarter.c19     \n",
            "  inflating: stories/quarter.c2      \n",
            "  inflating: stories/quarter.c3      \n",
            "  inflating: stories/quarter.c4      \n",
            "  inflating: stories/quarter.c5      \n",
            "  inflating: stories/quarter.c6      \n",
            "  inflating: stories/quarter.c7      \n",
            "  inflating: stories/quarter.c8      \n",
            "  inflating: stories/quarter.c9      \n",
            "  inflating: stories/quest           \n",
            "  inflating: stories/quickfix        \n",
            "  inflating: stories/quot            \n",
            "  inflating: stories/radar_ra.txt    \n",
            "  inflating: stories/rainda.txt      \n",
            "  inflating: stories/reality.txt     \n",
            "  inflating: stories/reap            \n",
            "  inflating: stories/redragon.txt    \n",
            "  inflating: stories/retrib.txt      \n",
            "  inflating: stories/rid.txt         \n",
            "  inflating: stories/robotech        \n",
            "  inflating: stories/rock            \n",
            "  inflating: stories/rocket.sf       \n",
            "  inflating: stories/roger1.txt      \n",
            "  inflating: stories/running.txt     \n",
            "  inflating: stories/s&m_plot        \n",
            "  inflating: stories/s&m_that        \n",
            "  inflating: stories/safe            \n",
            "  inflating: stories/sanpedr2.txt    \n",
            "  inflating: stories/shoscomb.txt    \n",
            "  inflating: stories/shrdfarm.txt    \n",
            "  inflating: stories/shulk.txt       \n",
            "  inflating: stories/sick-kid.txt    \n",
            "  inflating: stories/sight.txt       \n",
            "  inflating: stories/silverb.txt     \n",
            "  inflating: stories/sis             \n",
            "  inflating: stories/sleprncs.txt    \n",
            "  inflating: stories/snow.txt        \n",
            "  inflating: stories/snowmaid.txt    \n",
            "  inflating: stories/snowqn1.txt     \n",
            "  inflating: stories/social.vikings  \n",
            "  inflating: stories/socialvikings.txt  \n",
            "  inflating: stories/solitary.txt    \n",
            "  inflating: stories/space.txt       \n",
            "  inflating: stories/spam.key        \n",
            "  inflating: stories/spectacl.poe    \n",
            "  inflating: stories/spider.txt      \n",
            "  inflating: stories/spiders.txt     \n",
            "  inflating: stories/sqzply.txt      \n",
            "  inflating: stories/sre-dark.txt    \n",
            "  inflating: stories/stainles.ana    \n",
            "  inflating: stories/stairdre.txt    \n",
            "  inflating: stories/startrek.txt    \n",
            "  inflating: stories/stsgreek        \n",
            "  inflating: stories/sucker.txt      \n",
            "  inflating: stories/sunday.txt      \n",
            "  inflating: stories/superg1         \n",
            "  inflating: stories/szechuan        \n",
            "  inflating: stories/t_zone.jok      \n",
            "  inflating: stories/tailbear.txt    \n",
            "  inflating: stories/tao3.dos        \n",
            "  inflating: stories/taxnovel.txt    \n",
            "  inflating: stories/tcoa.txt        \n",
            "  inflating: stories/tctac.txt       \n",
            "  inflating: stories/tearglas.txt    \n",
            "  inflating: stories/telefone.txt    \n",
            "  inflating: stories/terrorbears.txt  \n",
            "  inflating: stories/testpilo.hum    \n",
            "  inflating: stories/textfile.primer  \n",
            "  inflating: stories/thanksg         \n",
            "  inflating: stories/the-tree.txt    \n",
            "  inflating: stories/thewave         \n",
            "  inflating: stories/timem.hac       \n",
            "  inflating: stories/times.fic       \n",
            "  inflating: stories/timetrav.txt    \n",
            "  inflating: stories/tin             \n",
            "  inflating: stories/tinsoldr.txt    \n",
            "  inflating: stories/toilet.s        \n",
            "  inflating: stories/traitor.txt     \n",
            "  inflating: stories/tree.txt        \n",
            "  inflating: stories/tuc_mees        \n",
            "  inflating: stories/uglyduck.txt    \n",
            "  inflating: stories/unluckwr.txt    \n",
            "  inflating: stories/vaincrow.txt    \n",
            "  inflating: stories/vainsong.txt    \n",
            "  inflating: stories/valen           \n",
            "  inflating: stories/vampword.txt    \n",
            "  inflating: stories/vday.hum        \n",
            "  inflating: stories/veiledl.txt     \n",
            "  inflating: stories/vgilante.txt    \n",
            "  inflating: stories/wall.art        \n",
            "  inflating: stories/wanderer.fun    \n",
            "  inflating: stories/weaver.txt      \n",
            "  inflating: stories/weeprncs.txt    \n",
            "  inflating: stories/whgdsreg.reg    \n",
            "  inflating: stories/wisteria.txt    \n",
            "  inflating: stories/withdraw.cyb    \n",
            "  inflating: stories/wlgirl.txt      \n",
            "  inflating: stories/wolf7kid.txt    \n",
            "  inflating: stories/wolfcran.txt    \n",
            "  inflating: stories/wolflamb.txt    \n",
            "  inflating: stories/wombat.und      \n",
            "  inflating: stories/write           \n",
            "  inflating: stories/wrt             \n",
            "  inflating: stories/yukon.txt       \n",
            "  inflating: stories/zombies.txt     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIDZzMWYYOmr",
        "colab_type": "code",
        "outputId": "021cebf6-cad2-4326-c13e-1e22df3d01d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "#Installing num2words library - It converts 42 to forty-two\n",
        "pip install num2words\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting num2words\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/eb/a2/ea800689730732e27711c41beed4b2a129b34974435bdc450377ec407738/num2words-0.5.10-py3-none-any.whl (101kB)\n",
            "\r\u001b[K     |███▎                            | 10kB 18.7MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 30kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 40kB 2.1MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 51kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 61kB 3.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 71kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 81kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 92kB 3.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 102kB 2.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.6/dist-packages (from num2words) (0.6.2)\n",
            "Installing collected packages: num2words\n",
            "Successfully installed num2words-0.5.10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQ8BcOg4XaQK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Importing required libraries \n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from collections import Counter\n",
        "from num2words import num2words\n",
        "\n",
        "import nltk\n",
        "import os\n",
        "import string\n",
        "import numpy as np\n",
        "import copy\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import re\n",
        "import math\n",
        "\n",
        "# %load_ext autotime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UieNg0enXaQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOzQH8CFXaQc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#declaring values of variables title and alpha\n",
        "\n",
        "title = \"stories\"\n",
        "alpha = 0.3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahmg8TVHXaQk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6YbHfwhXaQp",
        "colab_type": "text"
      },
      "source": [
        "## Taking all folders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vbhi9jBtXaQq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#code to traverse through all the folder directories\n",
        "\n",
        "folders = [x[0] for x in os.walk(str(os.getcwd())+'/'+title+'/')]\n",
        "folders[0] = folders[0][:len(folders[0])-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PK28787QXaQx",
        "colab_type": "code",
        "outputId": "089a4eab-b9ce-4421-efc6-52c807cd99ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "folders"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/stories', '/content/stories/SRE', '/content/stories/FARNON']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEl9f2sjXaQ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Y44xvXPXaQ_",
        "colab_type": "text"
      },
      "source": [
        "## Collecting the file names and titles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "7INrVR96XaRC",
        "colab_type": "code",
        "outputId": "9a20a2ea-e0d7-403f-8424-446c1aa3d670",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "#By traversing through the folders we are locating all the text files and adding all the text files into dataset\n",
        "\n",
        "dataset = []\n",
        "\n",
        "c = False\n",
        "\n",
        "for i in folders:\n",
        "    file = open(i+\"/index.html\", 'r')\n",
        "    text = file.read().strip()\n",
        "    file.close()\n",
        "\n",
        "    file_name = re.findall('><A HREF=\"(.*)\">', text)\n",
        "    file_title = re.findall('<BR><TD> (.*)\\n', text)\n",
        "\n",
        "    if c == False:\n",
        "        file_name = file_name[2:]\n",
        "        c = True\n",
        "        \n",
        "    print(len(file_name), len(file_title))\n",
        "\n",
        "    for j in range(len(file_name)):\n",
        "        dataset.append((str(i) +\"/\"+ str(file_name[j]), file_title[j]))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "452 452\n",
            "15 15\n",
            "0 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfQBshXlXaRN",
        "colab_type": "code",
        "outputId": "b419f4b1-a1ed-4674-8a2e-ed846d5704bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(dataset) #printing length of dataset\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "467"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2YAEzUZXaRW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N = len (dataset) #initializing variable n with value of length of Dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjY_7_qDXaRb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAosAR5qXaRe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_doc(id):\n",
        "    print(dataset[id])\n",
        "    file = open(dataset[id][0], 'r', encoding='cp1250')\n",
        "    text = file.read().strip()\n",
        "    file.close()\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-hs6Q5YXaRl",
        "colab_type": "code",
        "outputId": "47b25f5e-7159-4e05-cbdb-cbe69a2be9a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        }
      },
      "source": [
        "print(text)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<HTML>\n",
            "<TITLE>T E X T F I L E S</TITLE>\n",
            "<BODY BGCOLOR=\"#000000\" TEXT=\"#00FF00\" LINK=\"#00FF00\" ALINK=\"#00AA00\" VLINK=\"#00AA00\">\n",
            "<H1>Stories: TRISTAN FARNON</H1>\n",
            "<P>\n",
            "<TABLE WIDTH=100%>\n",
            "<TD BGCOLOR=#00FF00><FONT COLOR=#000000><B>Filename</B><BR></FONT></TD>\n",
            "<TD BGCOLOR=#00DD00><FONT COLOR=#000000><B>Size</B><BR></FONT></TD>\n",
            "<TD BGCOLOR=#00AA00><FONT COLOR=#000000><B>Description of the Textfile</B><BR></TD></TR>\n",
            "\n",
            "<tab indent=60 id=T><br>\n",
            "</TABLE><P><TABLE WIDTH=100%></TABLE><P>\n",
            "At the request of Tristan Farnon, this directory has been removed.\n",
            "<P>\n",
            "This saddens me no end, because his stories were truly wonderful. He'd\n",
            "post them on the Dark Side of the Moon AE, and while there were always\n",
            "interesting files being posted there, Tristan's always caused a rush of\n",
            "excitement through the place. You could tell he'd posted one, because \n",
            "the system would be really busy. \n",
            "<P>\n",
            "Tristan's stories were filled with irony, interesting situations, and\n",
            "well-written dialogue, all pleasant changes from what else you would find.\n",
            "They weren't interested in causing destruction or insulting others; they\n",
            "just entertained.\n",
            "<P>\n",
            "Tristan Farnon has gone on to do excellent web-based work on his site\n",
            "<A HREF=\"http://www.leisuretown.com\">Leisuretown</A>, which continues\n",
            "to display his excellent sense of humor, irony, characterization and\n",
            "flat-out quality. I give it my highest recommendation.\n",
            "<P>\n",
            "</BODY>\n",
            "</HTML>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNtL2lgOXaRs",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGkDuEyAXaRu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creating function to convert data to lower case\n",
        "def convert_lower_case(data):\n",
        "    return np.char.lower(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITxJdj1QXaRz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creating function to remove stop words from data\n",
        "def remove_stop_words(data):\n",
        "    stop_words = stopwords.words('english')\n",
        "    words = word_tokenize(str(data))\n",
        "    new_text = \"\"\n",
        "    for w in words:\n",
        "        if w not in stop_words and len(w) > 1:\n",
        "            new_text = new_text + \" \" + w\n",
        "    return new_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kC-hNmmMXaR6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#functions to remove punctuation\n",
        "\n",
        "def remove_punctuation(data):\n",
        "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
        "    for i in range(len(symbols)):\n",
        "        data = np.char.replace(data, symbols[i], ' ')\n",
        "        data = np.char.replace(data, \"  \", \" \")\n",
        "    data = np.char.replace(data, ',', '')\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aqj-TpXGXaR9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function to remove apostrophe\n",
        "def remove_apostrophe(data):\n",
        "    return np.char.replace(data, \"'\", \"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sr7xX3rVXaSC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function to reduce words from long to stem words - like removing \"-ing\" from creating to create word \"create\"\n",
        "\n",
        "def stemming(data):\n",
        "    stemmer= PorterStemmer()\n",
        "    \n",
        "    tokens = word_tokenize(str(data))\n",
        "    new_text = \"\"\n",
        "    for w in tokens:\n",
        "        new_text = new_text + \" \" + stemmer.stem(w)\n",
        "    return new_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRFSy6G9XaSK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#In this function we are using the num2words library\n",
        "\n",
        "def convert_numbers(data):\n",
        "    tokens = word_tokenize(str(data))\n",
        "    new_text = \"\"\n",
        "    for w in tokens:\n",
        "        try:\n",
        "            w = num2words(int(w))\n",
        "        except:\n",
        "            a = 0\n",
        "        new_text = new_text + \" \" + w\n",
        "    new_text = np.char.replace(new_text, \"-\", \" \")\n",
        "    return new_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioiDF3QDXaSP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qf2QlFE4XaSU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We created a function preprocess and passed data as argument. In this function we called all the preprocessing functions we defined above\n",
        "\n",
        "def preprocess(data):\n",
        "    data = convert_lower_case(data)\n",
        "    data = remove_punctuation(data) #remove comma seperately\n",
        "    data = remove_apostrophe(data)\n",
        "    data = remove_stop_words(data)\n",
        "    data = convert_numbers(data)\n",
        "    data = stemming(data)\n",
        "    data = remove_punctuation(data)\n",
        "    data = convert_numbers(data)\n",
        "    data = stemming(data) #needed again as we need to stem the words\n",
        "    data = remove_punctuation(data) #needed again as num2word is giving few hypens and commas fourty-one\n",
        "    data = remove_stop_words(data) #needed again as num2word is giving stop words 101 - one hundred and one\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kt-mpvvbXaSa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YoN9PaaXaSe",
        "colab_type": "text"
      },
      "source": [
        "## Extracting Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiY0c8dzZ9Xl",
        "colab_type": "code",
        "outputId": "fa153aca-2850-4bd7-d4ef-e76e90eeb969",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "#loading nltk library\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wq40A1q3aD3O",
        "colab_type": "code",
        "outputId": "12dd72cd-b6cd-4b00-98a1-3ccdfe95831b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "AhxfZ3K5XaSf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "processed_text = [] #creating empty array\n",
        "processed_title = [] #creating empty array\n",
        "\n",
        "for i in dataset[:N]: #traversing through the dataset\n",
        "    file = open(i[0], 'r', encoding=\"utf8\", errors='ignore')\n",
        "    text = file.read().strip()\n",
        "    file.close()\n",
        "\n",
        "    processed_text.append(word_tokenize(str(preprocess(text)))) #preprocessing the text by using the preprocessing function and appending it to processed_text\n",
        "    processed_title.append(word_tokenize(str(preprocess(i[1])))) #preprocessing the title by using the preprocessing function and appending it to processed_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMZZYVxOXaSi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJvdIU1pXaSo",
        "colab_type": "text"
      },
      "source": [
        "## Calculating DF for all words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "-1rxVaExXaSq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "DF = {} #Document Frequency variable \n",
        "\n",
        "for i in range(N):\n",
        "    tokens = processed_text[i]\n",
        "    for w in tokens:\n",
        "        try:\n",
        "            DF[w].add(i) #Adding tokens from text from dataset to DF array\n",
        "        except:\n",
        "            DF[w] = {i}\n",
        "\n",
        "    tokens = processed_title[i]\n",
        "    for w in tokens:\n",
        "        try:\n",
        "            DF[w].add(i) #Adding tokens from title from dataset to DF array\n",
        "        except:\n",
        "            DF[w] = {i}\n",
        "for i in DF:\n",
        "    DF[i] = len(DF[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "X60orClnXaSu",
        "colab_type": "code",
        "outputId": "3c2add89-9e6a-459f-ea5d-3b17afb4d80a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " DF"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sharewar': 5,\n",
              " 'trial': 35,\n",
              " 'project': 63,\n",
              " 'freewar': 1,\n",
              " 'need': 243,\n",
              " 'support': 87,\n",
              " 'continu': 193,\n",
              " 'one': 444,\n",
              " 'hundr': 329,\n",
              " 'west': 65,\n",
              " 'fifti': 160,\n",
              " 'three': 293,\n",
              " 'north': 73,\n",
              " 'jim': 20,\n",
              " 'prentic': 3,\n",
              " 'copyright': 115,\n",
              " 'thousand': 314,\n",
              " 'nine': 284,\n",
              " 'nineti': 215,\n",
              " 'brandon': 3,\n",
              " 'manitoba': 2,\n",
              " 'canada': 10,\n",
              " 'magic': 89,\n",
              " 'phrase': 40,\n",
              " 'spoken': 42,\n",
              " 'mumbl': 45,\n",
              " 'thought': 323,\n",
              " 'inwardli': 8,\n",
              " 'soul': 104,\n",
              " 'ventur': 57,\n",
              " 'northward': 7,\n",
              " 'imaginari': 20,\n",
              " 'line': 186,\n",
              " 'shown': 64,\n",
              " 'map': 48,\n",
              " 'label': 22,\n",
              " 'degr': 56,\n",
              " 'presenc': 83,\n",
              " 'indic': 74,\n",
              " 'highway': 35,\n",
              " 'travel': 109,\n",
              " 'road': 119,\n",
              " 'side': 243,\n",
              " 'sign': 151,\n",
              " 'divi': 29,\n",
              " 'territori': 21,\n",
              " 'distinct': 41,\n",
              " 'mind': 234,\n",
              " 'intern': 50,\n",
              " 'border': 37,\n",
              " 'writer': 55,\n",
              " 'poet': 17,\n",
              " 'pilot': 36,\n",
              " 'contribut': 31,\n",
              " 'lore': 6,\n",
              " 'rigor': 6,\n",
              " 'life': 274,\n",
              " 'bush': 45,\n",
              " 'told': 246,\n",
              " 'tale': 73,\n",
              " 'man': 271,\n",
              " 'eat': 139,\n",
              " 'mosquito': 6,\n",
              " 'murder': 81,\n",
              " 'hord': 15,\n",
              " 'black': 189,\n",
              " 'fli': 121,\n",
              " 'lump': 25,\n",
              " 'flesh': 71,\n",
              " 'carri': 176,\n",
              " 'away': 317,\n",
              " 'giant': 69,\n",
              " 'bull': 20,\n",
              " 'dog': 112,\n",
              " 'stori': 250,\n",
              " 'record': 95,\n",
              " 'break': 145,\n",
              " 'trout': 8,\n",
              " 'walley': 1,\n",
              " 'pike': 11,\n",
              " 'legion': 6,\n",
              " 'sight': 162,\n",
              " 'sound': 231,\n",
              " 'heard': 248,\n",
              " 'deep': 176,\n",
              " 'spruce': 8,\n",
              " 'forest': 80,\n",
              " 'crash': 80,\n",
              " 'moo': 8,\n",
              " 'tear': 130,\n",
              " 'brush': 72,\n",
              " 'tree': 149,\n",
              " 'drum': 37,\n",
              " 'grou': 4,\n",
              " 'incess': 11,\n",
              " 'hum': 47,\n",
              " 'insect': 24,\n",
              " 'cackl': 13,\n",
              " 'quackeri': 1,\n",
              " 'duck': 56,\n",
              " 'feed': 65,\n",
              " 'quiet': 119,\n",
              " 'pond': 18,\n",
              " 'placid': 9,\n",
              " 'bay': 45,\n",
              " 'intermitt': 7,\n",
              " 'song': 67,\n",
              " 'loon': 4,\n",
              " 'never': 315,\n",
              " 'forgotten': 82,\n",
              " 'voic': 220,\n",
              " 'signatur': 8,\n",
              " 'authent': 8,\n",
              " 'northern': 33,\n",
              " 'scene': 91,\n",
              " 'wildlif': 7,\n",
              " 'land': 152,\n",
              " 'seem': 267,\n",
              " 'differ': 171,\n",
              " 'found': 274,\n",
              " 'elsewh': 34,\n",
              " 'take': 319,\n",
              " 'special': 111,\n",
              " 'breed': 30,\n",
              " 'person': 184,\n",
              " 'live': 293,\n",
              " 'farther': 47,\n",
              " 'becom': 185,\n",
              " 'appar': 92,\n",
              " 'southern': 24,\n",
              " 'whether': 91,\n",
              " 'first': 311,\n",
              " 'fiftieth': 3,\n",
              " 'trip': 101,\n",
              " 'realli': 236,\n",
              " 'awar': 90,\n",
              " 'implic': 15,\n",
              " 'gener': 121,\n",
              " 'owner': 53,\n",
              " 'cottag': 25,\n",
              " 'lake': 44,\n",
              " 'amen': 8,\n",
              " 'hand': 299,\n",
              " 'weekend': 26,\n",
              " 'rough': 63,\n",
              " 'dweller': 10,\n",
              " 'year': 273,\n",
              " 'round': 153,\n",
              " 'basi': 28,\n",
              " 'modern': 40,\n",
              " 'cabin': 27,\n",
              " 'shore': 39,\n",
              " 'near': 159,\n",
              " 'larg': 209,\n",
              " 'metropolitan': 7,\n",
              " 'center': 90,\n",
              " 'equip': 72,\n",
              " 'electr': 61,\n",
              " 'servic': 96,\n",
              " 'telephon': 36,\n",
              " 'pave': 12,\n",
              " 'natur': 140,\n",
              " 'ga': 48,\n",
              " 'pipelin': 1,\n",
              " 'cabl': 31,\n",
              " 'televi': 36,\n",
              " 'nearbi': 77,\n",
              " 'provid': 80,\n",
              " 'food': 127,\n",
              " 'fuel': 30,\n",
              " 'repair': 40,\n",
              " 'entertain': 41,\n",
              " 'drivein': 1,\n",
              " 'theater': 14,\n",
              " 'fast': 124,\n",
              " 'chain': 74,\n",
              " 'abound': 17,\n",
              " 'waterfront': 6,\n",
              " 'busi': 165,\n",
              " 'dock': 28,\n",
              " 'built': 104,\n",
              " 'arriv': 158,\n",
              " 'boat': 48,\n",
              " 'shop': 81,\n",
              " 'laundri': 7,\n",
              " 'transfer': 35,\n",
              " 'suitca': 8,\n",
              " 'famili': 123,\n",
              " 'car': 108,\n",
              " 'local': 85,\n",
              " 'merchant': 23,\n",
              " 'deliv': 46,\n",
              " 'good': 299,\n",
              " 'water': 194,\n",
              " 'entrepreneur': 8,\n",
              " 'make': 320,\n",
              " 'businesss': 1,\n",
              " 'maintain': 43,\n",
              " 'absenc': 36,\n",
              " 'season': 36,\n",
              " 'locat': 65,\n",
              " 'five': 248,\n",
              " 'well': 348,\n",
              " 'street': 152,\n",
              " 'rival': 25,\n",
              " 'home': 253,\n",
              " 'mani': 249,\n",
              " 'citi': 113,\n",
              " 'commun': 77,\n",
              " 'although': 106,\n",
              " 'littl': 319,\n",
              " 'urban': 17,\n",
              " 'offer': 128,\n",
              " 'escap': 128,\n",
              " 'look': 372,\n",
              " 'averag': 30,\n",
              " 'studi': 99,\n",
              " 'compar': 54,\n",
              " 'standard': 44,\n",
              " 'area': 118,\n",
              " 'retreat': 44,\n",
              " 'bu': 30,\n",
              " 'doesnt': 122,\n",
              " 'start': 274,\n",
              " 'either': 160,\n",
              " 'walk': 256,\n",
              " 'call': 306,\n",
              " 'taxi': 17,\n",
              " 'ye': 214,\n",
              " 'kind': 185,\n",
              " 'anoth': 301,\n",
              " 'even': 321,\n",
              " 'ride': 89,\n",
              " 'back': 352,\n",
              " 'pick': 206,\n",
              " 'truck': 53,\n",
              " 'mail': 49,\n",
              " 'deliveri': 18,\n",
              " 'unknown': 80,\n",
              " 'pleasant': 60,\n",
              " 'stroll': 45,\n",
              " 'midsumm': 3,\n",
              " 'contrast': 26,\n",
              " 'ordeal': 17,\n",
              " 'threaten': 53,\n",
              " 'condit': 77,\n",
              " 'winter': 61,\n",
              " 'hou': 203,\n",
              " 'milk': 30,\n",
              " 'bread': 30,\n",
              " 'nonexist': 8,\n",
              " 'perhap': 140,\n",
              " 'newspap': 36,\n",
              " 'requir': 80,\n",
              " 'effort': 79,\n",
              " 'especi': 103,\n",
              " 'rail': 33,\n",
              " 'schedul': 29,\n",
              " 'air': 198,\n",
              " 'settlement': 11,\n",
              " 'sever': 172,\n",
              " 'restrict': 18,\n",
              " 'avail': 79,\n",
              " 'small': 223,\n",
              " 'compani': 99,\n",
              " 'exist': 104,\n",
              " 'fare': 19,\n",
              " 'collect': 100,\n",
              " 'transport': 44,\n",
              " 'nativ': 36,\n",
              " 'reserv': 80,\n",
              " 'diminish': 18,\n",
              " 'recent': 81,\n",
              " 'bushpilot': 1,\n",
              " 'charter': 8,\n",
              " 'aircraft': 8,\n",
              " 'still': 282,\n",
              " 'play': 196,\n",
              " 'role': 27,\n",
              " 'learn': 148,\n",
              " 'would': 363,\n",
              " 'keep': 226,\n",
              " 'colleagu': 14,\n",
              " 'ground': 169,\n",
              " 'summer': 87,\n",
              " 'float': 73,\n",
              " 'haul': 34,\n",
              " 'trapper': 2,\n",
              " 'fishermen': 8,\n",
              " 'freight': 9,\n",
              " 'fish': 82,\n",
              " 'fur': 30,\n",
              " 'suppli': 74,\n",
              " 'camp': 37,\n",
              " 'traplin': 2,\n",
              " 'log': 35,\n",
              " 'fall': 178,\n",
              " 'ice': 67,\n",
              " 'thin': 102,\n",
              " 'ski': 11,\n",
              " 'yet': 204,\n",
              " 'thick': 91,\n",
              " 'chang': 196,\n",
              " 'wheel': 68,\n",
              " 'permit': 40,\n",
              " 'present': 140,\n",
              " 'problem': 127,\n",
              " 'hear': 220,\n",
              " 'temperatur': 38,\n",
              " 'exceed': 16,\n",
              " 'forti': 148,\n",
              " 'zero': 52,\n",
              " 'blow': 128,\n",
              " 'snow': 40,\n",
              " 'crystal': 45,\n",
              " 'whiteout': 1,\n",
              " 'challeng': 42,\n",
              " 'daili': 47,\n",
              " 'work': 280,\n",
              " 'preheat': 1,\n",
              " 'engin': 72,\n",
              " 'coax': 14,\n",
              " 'congeal': 8,\n",
              " 'oil': 44,\n",
              " 'propel': 17,\n",
              " 'pitch': 60,\n",
              " 'mechan': 59,\n",
              " 'super': 44,\n",
              " 'cool': 95,\n",
              " 'gasolin': 10,\n",
              " 'regular': 58,\n",
              " 'chore': 8,\n",
              " 'must': 256,\n",
              " 'tie': 95,\n",
              " 'rope': 45,\n",
              " 'frozen': 47,\n",
              " 'hole': 111,\n",
              " 'four': 249,\n",
              " 'foot': 138,\n",
              " 'lift': 102,\n",
              " 'run': 222,\n",
              " 'onto': 128,\n",
              " 'board': 92,\n",
              " 'pole': 35,\n",
              " 'prevent': 52,\n",
              " 'freez': 27,\n",
              " 'passeng': 39,\n",
              " 'wear': 136,\n",
              " 'heavi': 136,\n",
              " 'arctic': 8,\n",
              " 'cloth': 137,\n",
              " 'heat': 87,\n",
              " 'system': 100,\n",
              " 'capabl': 59,\n",
              " 'cope': 12,\n",
              " 'cold': 165,\n",
              " 'batteri': 26,\n",
              " 'remov': 125,\n",
              " 'end': 268,\n",
              " 'day': 353,\n",
              " 'ensur': 18,\n",
              " 'maximum': 19,\n",
              " 'effici': 26,\n",
              " 'next': 250,\n",
              " 'wing': 61,\n",
              " 'cover': 172,\n",
              " 'stop': 237,\n",
              " 'ingress': 2,\n",
              " 'build': 151,\n",
              " 'frost': 19,\n",
              " 'flight': 63,\n",
              " 'surfac': 78,\n",
              " 'pot': 55,\n",
              " 'often': 121,\n",
              " 'get': 354,\n",
              " 'minu': 19,\n",
              " 'turn': 325,\n",
              " 'done': 200,\n",
              " 'chin': 44,\n",
              " 'coldsoak': 1,\n",
              " 'cessna': 2,\n",
              " 'flew': 92,\n",
              " 'airport': 18,\n",
              " 'lack': 68,\n",
              " 'comparison': 20,\n",
              " 'south': 79,\n",
              " 'usual': 159,\n",
              " 'oper': 87,\n",
              " 'larger': 67,\n",
              " 'may': 206,\n",
              " 'pay': 134,\n",
              " 'airstrip': 1,\n",
              " 'normal': 95,\n",
              " 'procedur': 23,\n",
              " 'buzz': 39,\n",
              " 'town': 122,\n",
              " 'let': 281,\n",
              " 'peopl': 272,\n",
              " 'know': 338,\n",
              " 'someon': 192,\n",
              " 'head': 306,\n",
              " 'strip': 60,\n",
              " 'meet': 160,\n",
              " 'unless': 89,\n",
              " 'custom': 66,\n",
              " 'transact': 14,\n",
              " 'cash': 46,\n",
              " 'chequ': 5,\n",
              " 'nearli': 114,\n",
              " 'useless': 43,\n",
              " 'villag': 53,\n",
              " 'without': 243,\n",
              " 'bank': 90,\n",
              " 'credit': 63,\n",
              " 'stranger': 75,\n",
              " 'foolhardi': 3,\n",
              " 'grip': 70,\n",
              " 'snowmobil': 3,\n",
              " 'major': 78,\n",
              " 'mode': 25,\n",
              " 'largest': 40,\n",
              " 'place': 275,\n",
              " 'vehicl': 39,\n",
              " 'left': 283,\n",
              " 'twenti': 213,\n",
              " 'hour': 208,\n",
              " 'allow': 130,\n",
              " 'restart': 5,\n",
              " 'balki': 1,\n",
              " 'resid': 54,\n",
              " 'enjoy': 131,\n",
              " 'month': 148,\n",
              " 'bring': 170,\n",
              " 'activ': 90,\n",
              " 'motor': 19,\n",
              " 'store': 98,\n",
              " 'lawnmow': 1,\n",
              " 'garden': 70,\n",
              " 'chair': 132,\n",
              " 'tune': 52,\n",
              " 'shack': 11,\n",
              " 'tow': 23,\n",
              " 'river': 71,\n",
              " 'blade': 39,\n",
              " 'power': 179,\n",
              " 'auger': 1,\n",
              " 'sharpen': 16,\n",
              " 'flock': 28,\n",
              " 'favorit': 58,\n",
              " 'spot': 135,\n",
              " 'drill': 19,\n",
              " 'feet': 191,\n",
              " 'comfort': 96,\n",
              " 'woodburn': 1,\n",
              " 'stove': 14,\n",
              " 'glow': 94,\n",
              " 'huddl': 30,\n",
              " 'lee': 19,\n",
              " 'catch': 121,\n",
              " 'tullib': 1,\n",
              " 'whitefish': 2,\n",
              " 'perch': 46,\n",
              " 'burbot': 1,\n",
              " 'catfish': 2,\n",
              " 'bass': 16,\n",
              " 'plenti': 58,\n",
              " 'popular': 51,\n",
              " 'bait': 15,\n",
              " 'minnow': 2,\n",
              " 'use': 278,\n",
              " 'sucker': 18,\n",
              " 'belli': 38,\n",
              " 'net': 54,\n",
              " 'bag': 86,\n",
              " 'egg': 49,\n",
              " 'metal': 96,\n",
              " 'spoon': 16,\n",
              " 'bucktail': 1,\n",
              " 'lead': 154,\n",
              " 'jig': 7,\n",
              " 'lure': 21,\n",
              " 'snell': 2,\n",
              " 'hook': 56,\n",
              " 'best': 204,\n",
              " 'whatev': 116,\n",
              " 'time': 407,\n",
              " 'hunt': 56,\n",
              " 'sport': 50,\n",
              " 'colder': 6,\n",
              " 'weather': 61,\n",
              " 'move': 230,\n",
              " 'warmth': 39,\n",
              " 'hunter': 30,\n",
              " 'face': 282,\n",
              " 'similar': 71,\n",
              " 'extrem': 68,\n",
              " 'snowmachin': 2,\n",
              " 'kept': 162,\n",
              " 'top': 180,\n",
              " 'depend': 63,\n",
              " 'death': 165,\n",
              " 'situat': 106,\n",
              " 'could': 358,\n",
              " 'develop': 91,\n",
              " 'machin': 91,\n",
              " 'mile': 102,\n",
              " 'rifl': 30,\n",
              " 'care': 238,\n",
              " 'bolt': 61,\n",
              " 'grea': 10,\n",
              " 'solid': 60,\n",
              " 'fire': 164,\n",
              " 'pin': 48,\n",
              " 'struck': 90,\n",
              " 'hammer': 25,\n",
              " 'bear': 94,\n",
              " 'weapon': 108,\n",
              " 'fail': 119,\n",
              " 'outdoor': 11,\n",
              " 'includ': 89,\n",
              " 'cross': 121,\n",
              " 'countri': 94,\n",
              " 'race': 106,\n",
              " 'team': 51,\n",
              " 'festiv': 14,\n",
              " 'held': 172,\n",
              " 'spring': 75,\n",
              " 'brought': 164,\n",
              " 'wager': 8,\n",
              " 'made': 302,\n",
              " 'date': 87,\n",
              " 'blower': 5,\n",
              " 'shovel': 11,\n",
              " 'skidoo': 2,\n",
              " 'suit': 108,\n",
              " 'replac': 76,\n",
              " 'lawn': 35,\n",
              " 'mower': 3,\n",
              " 'rake': 19,\n",
              " 'bath': 60,\n",
              " 'pussi': 6,\n",
              " 'willow': 6,\n",
              " 'blossom': 16,\n",
              " 'gee': 29,\n",
              " 'return': 213,\n",
              " 'frog': 22,\n",
              " 'begin': 161,\n",
              " 'croak': 17,\n",
              " 'battalion': 6,\n",
              " 'hatch': 31,\n",
              " 'low': 121,\n",
              " 'mercuri': 16,\n",
              " 'climb': 114,\n",
              " 'upward': 52,\n",
              " 'high': 182,\n",
              " 'reach': 245,\n",
              " 'sometim': 118,\n",
              " 'fantast': 27,\n",
              " 'differenti': 7,\n",
              " 'commerci': 27,\n",
              " 'trap': 68,\n",
              " 'construct': 54,\n",
              " 'landscap': 30,\n",
              " 'patrol': 24,\n",
              " 'hydro': 1,\n",
              " 'transmiss': 20,\n",
              " 'interv': 36,\n",
              " 'conservationist': 1,\n",
              " 'anim': 110,\n",
              " 'censu': 1,\n",
              " 'tank': 30,\n",
              " 'babi': 74,\n",
              " 'restock': 2,\n",
              " 'seriou': 95,\n",
              " 'ill': 252,\n",
              " 'injur': 35,\n",
              " 'taken': 178,\n",
              " 'medic': 50,\n",
              " 'medevac': 1,\n",
              " 'tourist': 13,\n",
              " 'prospector': 2,\n",
              " 'vie': 5,\n",
              " 'geologist': 3,\n",
              " 'botanist': 1,\n",
              " 'biologist': 3,\n",
              " 'entomologist': 1,\n",
              " 'surveyor': 1,\n",
              " 'see': 357,\n",
              " 'touch': 162,\n",
              " 'smell': 104,\n",
              " 'measur': 60,\n",
              " 'wonder': 211,\n",
              " 'open': 272,\n",
              " 'herald': 12,\n",
              " 'recreat': 11,\n",
              " 'type': 109,\n",
              " 'trailer': 11,\n",
              " 'van': 26,\n",
              " 'motorhom': 2,\n",
              " '4x4': 1,\n",
              " 'bicycl': 17,\n",
              " 'motorcycl': 10,\n",
              " 'atv': 1,\n",
              " 'timer': 5,\n",
              " 'fill': 184,\n",
              " 'privat': 82,\n",
              " 'provinci': 5,\n",
              " 'govern': 61,\n",
              " 'campground': 2,\n",
              " 'experienc': 47,\n",
              " 'adventuresom': 1,\n",
              " 'trail': 63,\n",
              " 'stream': 78,\n",
              " 'park': 89,\n",
              " 'lot': 163,\n",
              " 'cram': 15,\n",
              " 'booz': 10,\n",
              " 'sold': 53,\n",
              " 'great': 240,\n",
              " 'quantiti': 20,\n",
              " 'hotel': 38,\n",
              " 'full': 210,\n",
              " 'advanc': 82,\n",
              " 'prior': 11,\n",
              " 'leav': 246,\n",
              " 'short': 168,\n",
              " 'week': 151,\n",
              " 'prime': 30,\n",
              " 'period': 56,\n",
              " 'everi': 249,\n",
              " 'occupi': 48,\n",
              " 'sportsmen': 1,\n",
              " 'rece': 12,\n",
              " 'cycl': 30,\n",
              " 'repeat': 94,\n",
              " 'annual': 16,\n",
              " 'number': 148,\n",
              " 'piper': 6,\n",
              " 'cub': 5,\n",
              " 'beechcraft': 2,\n",
              " 'taildragg': 1,\n",
              " 'bizjet': 1,\n",
              " 'new': 250,\n",
              " 'experi': 112,\n",
              " 'unnerv': 10,\n",
              " 'accustom': 29,\n",
              " 'network': 37,\n",
              " 'railroad': 10,\n",
              " 'track': 94,\n",
              " 'alway': 275,\n",
              " 'contact': 78,\n",
              " 'navig': 19,\n",
              " 'aid': 72,\n",
              " 'seldom': 30,\n",
              " 'prepar': 117,\n",
              " 'realiti': 67,\n",
              " 'apart': 124,\n",
              " 'case': 142,\n",
              " 'railway': 14,\n",
              " 'lower': 99,\n",
              " 'altitud': 11,\n",
              " 'except': 160,\n",
              " 'rather': 146,\n",
              " 'rule': 79,\n",
              " 'plan': 157,\n",
              " 'cour': 203,\n",
              " 'mandatori': 5,\n",
              " 'read': 188,\n",
              " 'difficult': 78,\n",
              " 'basic': 43,\n",
              " 'shape': 122,\n",
              " 'sharp': 92,\n",
              " 'eye': 300,\n",
              " 'seaplan': 1,\n",
              " 'ever': 270,\n",
              " 'danger': 119,\n",
              " 'rock': 115,\n",
              " 'reef': 7,\n",
              " 'amplifi': 10,\n",
              " 'distanc': 118,\n",
              " 'civil': 49,\n",
              " 'around': 292,\n",
              " 'suitabl': 21,\n",
              " 'refuel': 4,\n",
              " 'facil': 28,\n",
              " 'element': 36,\n",
              " 'pleasur': 74,\n",
              " 'remot': 42,\n",
              " 'green': 146,\n",
              " 'undisturb': 10,\n",
              " 'indescrib': 10,\n",
              " 'secur': 92,\n",
              " 'set': 245,\n",
              " 'true': 138,\n",
              " 'beauti': 160,\n",
              " 'teem': 5,\n",
              " 'surround': 91,\n",
              " 'wood': 108,\n",
              " 'smoke': 116,\n",
              " 'coff': 70,\n",
              " 'mingl': 22,\n",
              " 'fresh': 77,\n",
              " 'sizzl': 14,\n",
              " 'frypan': 1,\n",
              " 'bird': 92,\n",
              " 'cri': 184,\n",
              " 'wail': 42,\n",
              " 'coyot': 6,\n",
              " 'wolv': 11,\n",
              " 'whistl': 49,\n",
              " 'raven': 17,\n",
              " 'hawk': 17,\n",
              " 'eagl': 17,\n",
              " 'shorelin': 5,\n",
              " 'wake': 87,\n",
              " 'pass': 214,\n",
              " 'beaver': 6,\n",
              " 'muskrat': 1,\n",
              " 'occas': 77,\n",
              " 'warn': 104,\n",
              " 'smack': 25,\n",
              " 'tail': 71,\n",
              " 'sen': 150,\n",
              " 'vari': 30,\n",
              " 'hue': 15,\n",
              " 'evergreen': 6,\n",
              " 'decidu': 2,\n",
              " 'color': 96,\n",
              " 'windflow': 1,\n",
              " 'tast': 94,\n",
              " 'wild': 94,\n",
              " 'strawberri': 9,\n",
              " 'raspberri': 5,\n",
              " 'blueberri': 5,\n",
              " 'excit': 110,\n",
              " 'sky': 118,\n",
              " 'danc': 86,\n",
              " 'dazzl': 20,\n",
              " 'display': 59,\n",
              " 'aurora': 6,\n",
              " 'boreali': 2,\n",
              " 'appear': 196,\n",
              " 'light': 240,\n",
              " 'star': 108,\n",
              " 'incr': 68,\n",
              " 'brilliant': 62,\n",
              " 'smog': 7,\n",
              " 'free': 140,\n",
              " 'heaven': 80,\n",
              " 'night': 252,\n",
              " 'lie': 161,\n",
              " 'tent': 33,\n",
              " 'rustl': 29,\n",
              " 'noi': 99,\n",
              " 'twig': 12,\n",
              " 'snap': 85,\n",
              " 'mou': 31,\n",
              " 'rabbit': 27,\n",
              " 'visual': 29,\n",
              " 'bundl': 27,\n",
              " 'branch': 65,\n",
              " 'wind': 139,\n",
              " 'rise': 109,\n",
              " 'wave': 138,\n",
              " 'airplan': 8,\n",
              " 'alright': 30,\n",
              " 'go': 361,\n",
              " 'check': 134,\n",
              " 'like': 390,\n",
              " 'morn': 182,\n",
              " 'chanc': 150,\n",
              " 'big': 214,\n",
              " 'lost': 205,\n",
              " 'bigger': 47,\n",
              " 'lone': 86,\n",
              " 'ear': 134,\n",
              " 'somehow': 87,\n",
              " 'penetr': 50,\n",
              " 'last': 283,\n",
              " 'thing': 310,\n",
              " 'sleep': 145,\n",
              " 'peac': 97,\n",
              " 'earli': 123,\n",
              " 'announc': 73,\n",
              " 'constant': 35,\n",
              " 'sting': 25,\n",
              " 'slap': 55,\n",
              " 'warfar': 9,\n",
              " 'might': 217,\n",
              " 'far': 220,\n",
              " 'friend': 234,\n",
              " 'neighbor': 45,\n",
              " 'also': 204,\n",
              " 'howl': 47,\n",
              " 'scream': 190,\n",
              " 'kid': 111,\n",
              " 'squeal': 31,\n",
              " 'tire': 123,\n",
              " 'polic': 84,\n",
              " 'siren': 23,\n",
              " 'reek': 12,\n",
              " 'diesel': 7,\n",
              " 'fume': 22,\n",
              " 'buss': 6,\n",
              " 'jangl': 7,\n",
              " 'typewrit': 9,\n",
              " 'clatter': 27,\n",
              " 'pile': 89,\n",
              " 'dissatisfi': 7,\n",
              " 'grumbl': 31,\n",
              " 'boss': 45,\n",
              " 'loom': 37,\n",
              " 'threateningli': 3,\n",
              " 'jostl': 8,\n",
              " 'lunch': 62,\n",
              " 'fight': 120,\n",
              " 'traffic': 49,\n",
              " 'deposit': 31,\n",
              " 'children': 104,\n",
              " 'toy': 46,\n",
              " 'clutter': 14,\n",
              " 'driveway': 29,\n",
              " 'sit': 193,\n",
              " 'tv': 47,\n",
              " 'dinner': 80,\n",
              " 'discuss': 75,\n",
              " 'crazi': 68,\n",
              " 'happen': 231,\n",
              " 'dealer': 16,\n",
              " 'order': 174,\n",
              " 'part': 218,\n",
              " 'ship': 88,\n",
              " 'plane': 37,\n",
              " 'meanwhil': 62,\n",
              " 'appli': 54,\n",
              " 'applianc': 9,\n",
              " 'tool': 56,\n",
              " 'repairman': 5,\n",
              " 'send': 137,\n",
              " 'buy': 99,\n",
              " 'station': 97,\n",
              " 'radio': 46,\n",
              " 'cbc': 1,\n",
              " 'augment': 9,\n",
              " 'program': 72,\n",
              " 'obtain': 42,\n",
              " 'exampl': 58,\n",
              " 'veterinarian': 2,\n",
              " 'visit': 117,\n",
              " 'weekli': 9,\n",
              " 'monthli': 11,\n",
              " 'extent': 18,\n",
              " 'injuri': 34,\n",
              " 'distant': 69,\n",
              " 'meat': 46,\n",
              " 'produc': 78,\n",
              " 'acut': 14,\n",
              " 'howev': 172,\n",
              " 'access': 51,\n",
              " 'event': 86,\n",
              " 'shipment': 8,\n",
              " 'perish': 21,\n",
              " 'commod': 9,\n",
              " 'space': 113,\n",
              " 'subsequ': 15,\n",
              " 'price': 74,\n",
              " 'reflect': 106,\n",
              " 'ad': 116,\n",
              " 'cost': 69,\n",
              " 'product': 65,\n",
              " 'subject': 110,\n",
              " 'propan': 2,\n",
              " 'seven': 179,\n",
              " 'consid': 143,\n",
              " 'expen': 61,\n",
              " 'higher': 67,\n",
              " 'ago': 168,\n",
              " 'ran': 194,\n",
              " 'bed': 151,\n",
              " 'per': 48,\n",
              " 'breakfast': 56,\n",
              " 'toast': 21,\n",
              " 'bacon': 11,\n",
              " 'twelv': 135,\n",
              " 'avga': 2,\n",
              " 'gallon': 10,\n",
              " 'two': 345,\n",
              " 'circl': 95,\n",
              " 'expect': 158,\n",
              " 'matter': 183,\n",
              " 'ancestri': 3,\n",
              " 'white': 184,\n",
              " 'born': 87,\n",
              " 'parent': 84,\n",
              " 'employ': 80,\n",
              " 'reason': 148,\n",
              " 'transient': 2,\n",
              " 'follow': 236,\n",
              " 'job': 140,\n",
              " 'corpor': 32,\n",
              " 'definit': 75,\n",
              " 'endur': 41,\n",
              " 'involv': 92,\n",
              " 'give': 255,\n",
              " 'extend': 71,\n",
              " 'ten': 210,\n",
              " 'adapt': 24,\n",
              " 'way': 339,\n",
              " 'abl': 183,\n",
              " 'independ': 26,\n",
              " 'adjust': 47,\n",
              " 'adult': 29,\n",
              " 'took': 271,\n",
              " 'stock': 48,\n",
              " 'stuck': 83,\n",
              " 'rut': 7,\n",
              " 'townspeopl': 4,\n",
              " 'post': 75,\n",
              " 'offic': 124,\n",
              " 'decid': 213,\n",
              " 'possibl': 174,\n",
              " 'remain': 168,\n",
              " 'rest': 207,\n",
              " 'februari': 11,\n",
              " 'eighti': 143,\n",
              " 'miss': 145,\n",
              " 'delici': 24,\n",
              " 'clear': 167,\n",
              " 'steak': 12,\n",
              " 'moosemeat': 1,\n",
              " 'sausag': 14,\n",
              " 'pancak': 6,\n",
              " 'freedom': 37,\n",
              " 'afford': 56,\n",
              " 'seclud': 13,\n",
              " 'within': 164,\n",
              " 'superb': 6,\n",
              " 'friendship': 28,\n",
              " 'camaraderi': 2,\n",
              " 'happi': 137,\n",
              " 'opportun': 59,\n",
              " 'approach': 133,\n",
              " 'midpoint': 1,\n",
              " 'want': 313,\n",
              " 'supermarket': 13,\n",
              " 'mall': 22,\n",
              " 'fanci': 61,\n",
              " 'restaur': 33,\n",
              " 'gourmet': 2,\n",
              " 'closer': 107,\n",
              " 'drive': 122,\n",
              " 'old': 281,\n",
              " 'broader': 7,\n",
              " 'choic': 87,\n",
              " 'channel': 46,\n",
              " 'east': 61,\n",
              " 'besid': 153,\n",
              " 'music': 80,\n",
              " 'nation': 49,\n",
              " 'fourteen': 96,\n",
              " 'sli': 15,\n",
              " 'fox': 24,\n",
              " 'mr': 120,\n",
              " 'sat': 197,\n",
              " 'front': 204,\n",
              " 'porch': 25,\n",
              " 'carrot': 5,\n",
              " 'come': 341,\n",
              " 'said': 348,\n",
              " 'shade': 59,\n",
              " 'paw': 43,\n",
              " 'exclaim': 86,\n",
              " 'across': 196,\n",
              " 'yard': 95,\n",
              " 'repli': 180,\n",
              " 'slight': 72,\n",
              " 'frown': 59,\n",
              " 'havent': 94,\n",
              " 'seen': 230,\n",
              " 'long': 307,\n",
              " 'chat': 31,\n",
              " 'rude': 45,\n",
              " 'enemi': 80,\n",
              " 'seat': 130,\n",
              " 'polit': 70,\n",
              " 'stay': 165,\n",
              " 'ask': 281,\n",
              " 'mother': 126,\n",
              " 'tonight': 70,\n",
              " 'wont': 160,\n",
              " 'rab': 1,\n",
              " 'oh': 211,\n",
              " 'us': 234,\n",
              " 'pretend': 49,\n",
              " 'disappoint': 46,\n",
              " 'sorri': 140,\n",
              " 'engag': 61,\n",
              " 'today': 99,\n",
              " 'tomorrow': 86,\n",
              " 'chuckl': 72,\n",
              " ...}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDnmzzmOXaSw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4JSVWEYXaS2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_vocab_size = len(DF) #assigning length of data frequency array to total vocab size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e4SvmjBXaS7",
        "colab_type": "code",
        "outputId": "a5117b3e-34fb-4c05-a70c-f33a0a73f78e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "total_vocab_size"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32350"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "Mpk9eycGXaTB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_vocab = [x for x in DF] #assigning token one by one to the total_vocab variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rx9Z4VwlXaTD",
        "colab_type": "code",
        "outputId": "dca3be74-b138-4638-c1c3-0052df5548ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(total_vocab[:20])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['sharewar', 'trial', 'project', 'freewar', 'need', 'support', 'continu', 'one', 'hundr', 'west', 'fifti', 'three', 'north', 'jim', 'prentic', 'copyright', 'thousand', 'nine', 'nineti', 'brandon']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJ6ig1cNXaTH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdFFw4RWXaTM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# defining doc_freq function and passing the words as argument\n",
        "def doc_freq(word):\n",
        "    c = 0\n",
        "    try:\n",
        "        c = DF[word]\n",
        "    except:\n",
        "        pass\n",
        "    return c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZXRvWQEXaTO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5pd16vMXaTR",
        "colab_type": "text"
      },
      "source": [
        "### Calculating TF-IDF for body, we will consider this as the actual tf-idf as we will add the title weight to this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "eTUlwAGQXaTS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc = 0\n",
        "\n",
        "tf_idf = {}\n",
        "\n",
        "for i in range(N):\n",
        "    \n",
        "    tokens = processed_text[i]\n",
        "    \n",
        "    counter = Counter(tokens + processed_title[i])\n",
        "    words_count = len(tokens + processed_title[i])\n",
        "    \n",
        "    for token in np.unique(tokens):\n",
        "        \n",
        "        tf = counter[token]/words_count #calculating the value of tf\n",
        "        df = doc_freq(token) #Calculating the value of df using function doc_freq\n",
        "        idf = np.log((N+1)/(df+1)) #calculating the idf using the log function\n",
        "        \n",
        "        tf_idf[doc, token] = tf*idf\n",
        "\n",
        "    doc += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbZqMwPyXaTU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tf_idf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTlmuB2hXaTY",
        "colab_type": "text"
      },
      "source": [
        "### Calculating TF-IDF for Title"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asUuOV58XaTa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Performing same operations as above but this time for the title dataset\n",
        "doc = 0\n",
        "\n",
        "tf_idf_title = {}\n",
        "\n",
        "for i in range(N):\n",
        "    \n",
        "    tokens = processed_title[i]\n",
        "    counter = Counter(tokens + processed_text[i])\n",
        "    words_count = len(tokens + processed_text[i])\n",
        "\n",
        "    for token in np.unique(tokens):\n",
        "        \n",
        "        tf = counter[token]/words_count\n",
        "        df = doc_freq(token)\n",
        "        idf = np.log((N+1)/(df+1)) #numerator is added 1 to avoid negative values\n",
        "        \n",
        "        tf_idf_title[doc, token] = tf*idf\n",
        "\n",
        "    doc += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "fwEuekccXaTe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tf_idf_title"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctfndouVXaTi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWStm-AoXaTl",
        "colab_type": "code",
        "outputId": "cdc6fd41-71c2-4c72-8456-5470216285c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tf_idf[(0,\"go\")] #Checking the value of the token go and calculating its idf value"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0002906893990853149"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUaOwJZdXaT5",
        "colab_type": "code",
        "outputId": "00562734-fbaf-41ac-8224-e6bda0f8a6c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tf_idf_title[(0,\"go\")] #Checking the value of the token \"go\" in title dataset and calculating its idf value"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0002906893990853149"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QC5fvuptXaT-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLiWdTWbXaUB",
        "colab_type": "text"
      },
      "source": [
        "## Merging the TF-IDF according to weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyrdIVCYXaUB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#using weights with the help of variable alpha which we set manual value in the beginning\n",
        "for i in tf_idf:\n",
        "    tf_idf[i] *= alpha"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziV2y-WAXaUE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in tf_idf_title:\n",
        "    tf_idf[i] = tf_idf_title[i]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lh2Z0oBNXaUG",
        "colab_type": "code",
        "outputId": "d70a29c3-f9f6-4824-c1b7-071d66455f5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(tf_idf)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "344378"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1GxS992XaUK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3asfbCoXaUP",
        "colab_type": "text"
      },
      "source": [
        "# TF-IDF Matching Score Ranking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oN8Ym5zXaUR",
        "colab_type": "code",
        "outputId": "d6d574bc-c8b0-4e1b-bae8-c4f29bd4b831",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "source": [
        "#Maching the score using the weights \n",
        "\n",
        "def matching_score(k, query):\n",
        "    preprocessed_query = preprocess(query)\n",
        "    tokens = word_tokenize(str(preprocessed_query))\n",
        "\n",
        "    print(\"Matching Score\")\n",
        "    print(\"\\nQuery:\", query)\n",
        "    print(\"\")\n",
        "    print(tokens)\n",
        "    \n",
        "    query_weights = {}\n",
        "\n",
        "    for key in tf_idf: #traversing through tf_idf\n",
        "        # If the word is in the dictionary \n",
        "        #adding word to tf_idf list\n",
        "        if key[1] in tokens: \n",
        "            try:\n",
        "                query_weights[key[0]] += tf_idf[key]\n",
        "                #if the word is found at the first time\n",
        "            except:\n",
        "                query_weights[key[0]] = tf_idf[key]\n",
        "    \n",
        "    query_weights = sorted(query_weights.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    print(\"\")\n",
        "    \n",
        "    l = []\n",
        "    \n",
        "    for i in query_weights[:10]:\n",
        "        l.append(i[0])\n",
        "    \n",
        "    print(l)\n",
        "    \n",
        "#Depending on the value of the tf_idf the top 10 are ranked\n",
        "matching_score(10, \"Without the drive of Rebeccah's insistence, Kate lost her momentum. She stood next a slatted oak bench, canisters still clutched, surveying\")"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Matching Score\n",
            "\n",
            "Query: Without the drive of Rebeccah's insistence, Kate lost her momentum. She stood next a slatted oak bench, canisters still clutched, surveying\n",
            "\n",
            "['without', 'drive', 'rebeccah', 'insist', 'kate', 'lost', 'momentum', 'stood', 'next', 'slat', 'oak', 'bench', 'canist', 'still', 'clutch', 'survey']\n",
            "\n",
            "[166, 200, 352, 433, 211, 350, 175, 187, 188, 294]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fe3MnpVlXaUT",
        "colab_type": "code",
        "outputId": "238230f3-0071-41ee-e81f-40a4e4395ed0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print_doc(166)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('/content/stories/fea3', 'Survey Results, by Joe DeRouen (1994)')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgTpwoykXaUV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URSycvSXXaUY",
        "colab_type": "text"
      },
      "source": [
        "# TF-IDF Cosine Similarity Ranking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UHP27iNXaUZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define cosin similarity\n",
        "def cosine_sim(a, b):\n",
        "    cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
        "    return cos_sim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p92KfTRUXaUd",
        "colab_type": "text"
      },
      "source": [
        "### Vectorising tf-idf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDJvr_hwXaUf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Calculating dimension \n",
        "D = np.zeros((N, total_vocab_size))\n",
        "for i in tf_idf:\n",
        "    try:\n",
        "        ind = total_vocab.index(i[1])\n",
        "        D[i[0]][ind] = tf_idf[i]\n",
        "    except:\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKV4qYW0XaUg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Generates number representation for token\n",
        "\n",
        "def gen_vector(tokens):\n",
        "\n",
        "    Q = np.zeros((len(total_vocab)))\n",
        "    \n",
        "    counter = Counter(tokens)\n",
        "    words_count = len(tokens)\n",
        "\n",
        "    query_weights = {}\n",
        "    \n",
        "    for token in np.unique(tokens):\n",
        "        \n",
        "        tf = counter[token]/words_count\n",
        "        df = doc_freq(token)\n",
        "        idf = math.log((N+1)/(df+1))\n",
        "\n",
        "        try:\n",
        "            ind = total_vocab.index(token)\n",
        "            Q[ind] = tf*idf\n",
        "        except:\n",
        "            pass\n",
        "    return Q"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydOlxW8NXaUj",
        "colab_type": "code",
        "outputId": "ae769c6f-f5af-45b3-d2d3-7998f096bb08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "source": [
        "#calculating cosin similarity and finding the matching documents\n",
        "def cosine_similarity(k, query):\n",
        "    print(\"Cosine Similarity\")\n",
        "    preprocessed_query = preprocess(query)\n",
        "    tokens = word_tokenize(str(preprocessed_query))\n",
        "    \n",
        "    print(\"\\nQuery:\", query)\n",
        "    print(\"\")\n",
        "    print(tokens)\n",
        "    \n",
        "    d_cosines = []\n",
        "    \n",
        "    query_vector = gen_vector(tokens)\n",
        "    \n",
        "    for d in D:\n",
        "        d_cosines.append(cosine_sim(query_vector, d))\n",
        "        \n",
        "    out = np.array(d_cosines).argsort()[-k:][::-1]\n",
        "    \n",
        "    print(\"\")\n",
        "    \n",
        "    print(out)\n",
        "\n",
        "#     for i in out:\n",
        "#         print(i, dataset[i][0])\n",
        "\n",
        "Q = cosine_similarity(10, \"Without the drive of Rebeccah's insistence, Kate lost her momentum. She stood next a slatted oak bench, canisters still clutched, surveying\")"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cosine Similarity\n",
            "\n",
            "Query: Without the drive of Rebeccah's insistence, Kate lost her momentum. She stood next a slatted oak bench, canisters still clutched, surveying\n",
            "\n",
            "['without', 'drive', 'rebeccah', 'insist', 'kate', 'lost', 'momentum', 'stood', 'next', 'slat', 'oak', 'bench', 'canist', 'still', 'clutch', 'survey']\n",
            "\n",
            "[200 166 433 175 169 402 211  87 151 369]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYvlz0X6XaUp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3dlpdTFXaUr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6jaknV-XaUw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "gt1dc-BcXaUw",
        "colab_type": "code",
        "outputId": "92088056-6b85-4b2a-f0e3-b39b6eb61684",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print_doc(200)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('/content/stories/ghost', 'Time for Flowers, by Gay Bost')\n",
            "TIME FOR FLOWERS\n",
            "  by Gay Bost\n",
            "\n",
            "They'd put flowers up. She hadn't noticed. Time wouldn't hold still.\n",
            "She remembered, quite clearly, that time had been a simple thing; one\n",
            "moment following the previous one, seconds strung out neatly like her\n",
            "mother's pearls laid out on the dark mahogany vanity each Sunday\n",
            "morning. But there had been a catch . . . \n",
            "\n",
            "Hung around Mother's neck the catch clicked and the tidy little line \n",
            "of seconds became a never ending circle with only the catch in the \n",
            "middle. For some reason the thought of pearls gathered from the sea, \n",
            "naturally nested within the confines of oyster shells, scattered \n",
            "haphazardly about the ocean floor disturbed her.\n",
            "\n",
            "Now they'd put up the flowers in the same careless groupings. This,\n",
            "too, disturbed her. Bright yellow trumpets, their collars spread to\n",
            "catch the sun, dotted the front yard in clusters of two or three, five\n",
            "or six. Bunches laid carelessly and forgotten. In a moment she'd\n",
            "come away from the window and have a word with the gardener. He\n",
            "listened so well and explained to others so reasonably why this should\n",
            "be so instead of the way they wanted it done, how that would look\n",
            "better or cut the wind more effectively.\n",
            "\n",
            "And then she recalled his stiff body stretched out in the little bed\n",
            "over the garages. Another pearl had come loose from the strand,\n",
            "seeming to want to search out its old home in a far away oyster bed.\n",
            "She would have those pearls laid out neatly, one following the one\n",
            "before and so on and so on. She would have those damned yellow\n",
            "flowers marching smartly along the walk. She'd have it if she\n",
            "had to go out there and replant each and every one of them.\n",
            "\n",
            "She flew down the hallway and sailed over the steps leading the\n",
            "back way to the kitchen, much as she had done as a child. Where then\n",
            "she had skipped in joy she now catapulted her form in anger.\n",
            "\n",
            "\"And there you are!\" she said, as she encountered the woman she had \n",
            "come to know as Kate. All of five foot tall in her stocking feet and \n",
            "surely every bit of two hundred pounds, her pudgy fists more often \n",
            "than not braced on the sudden outburst of her hips. So she stood, \n",
            "having turned from the sink. Suds and water darkened the fabric of her \n",
            "dress. Her face was pleasant; round, rosy cheeked, with eyes the color \n",
            "of mint in the summer sunset. \"And *where have you been these three days*?\"\n",
            "\n",
            "\"I want the flowers straightened out,\" Rebeccah said. \"I want the\n",
            "flowers placed in the proper alignments.\"\n",
            "\n",
            "Kate tilted her head, narrowed her eyes and frowned. \"Ah, you're in a\n",
            "huff again. What can it be this time?\"\n",
            "\n",
            "\"I want the flours straightened out,\" Rebeccah yelled, coming up to\n",
            "the woman's face.\n",
            "\n",
            "Kate went directly to the cupboard, strained upon her tiny toes to\n",
            "reach the second shelf, and pulled the flour canister out. She set it\n",
            "on the counter. She repeated the process, bringing out a smaller\n",
            "canister. Rebecca knew this one to be the unbleached flour Kate used\n",
            "for one particular recipe.\n",
            "\n",
            "\"No,no, no!\"  Rebeccah hissed. \"Flowers!  Not flours!\"  She propped\n",
            "herself against the edge of the kitchen table and crossed her arms\n",
            "over her chest, waiting for the woman to get it right.\n",
            "\n",
            "Kate stood looking dumbly at the canisters. \"Now, what was I going to\n",
            "do with these?\"  she asked herself. She drummed her fingers on the\n",
            "counter top before bringing one hand to her lips, where the pointer\n",
            "finger tapped on her upper lip.\n",
            "\n",
            "\"The Flowers!  Outside!\"  Rebecca screamed, highly agitated.\n",
            "\n",
            "Kate gathered the two canisters and moved toward the back door, one\n",
            "held against her ample form by each arm.\n",
            "\n",
            "Exasperated, Rebeccah followed her out, watching to see what she would do.\n",
            "\n",
            "Without the drive of Rebeccah's insistence, Kate lost her momentum.\n",
            "She stood next a slatted oak bench, canisters still clutched, surveying \n",
            "the sunlit yard and gardens beyond. Harold had done a passable job \n",
            "trimming the hedges, but Kate missed the gardener's touch. She resolved \n",
            "to contact the nursery and find another. Flaux, bright purples, pinks \n",
            "and radiant white encircled the herb garden, a brilliant contrast to \n",
            "the varied greens within. She set the canisters down on the bench and \n",
            "moved toward the cheerful scene.\n",
            "\n",
            "Rebeccah, discouraged, sat primly on the edge of the bench, dusting a\n",
            "wisp of hair away from her temple. New mint, dew draped, veiled a\n",
            "border of stocky wooden poles to trail onto the walk, had been crushed, \n",
            "probably by the man of the house on his way off to work. The scent \n",
            "filled her nostrils. She found herself a child, again, tasting her \n",
            "first tea with mint -- fresh cut from the gardens. _\"How long has it\n",
            "been?\"_  she wondered. Kate had gone down on her knees over the flaux,\n",
            "bending to weed through the thyme.\n",
            "\n",
            "\"I don't know why I have to put up with idiots,\" Rebeccah complained.\n",
            "\"It all so worthless, so futile.\"  With a great sigh she rose from the\n",
            "bench and made her way back into the house. The bright kitchen seemed\n",
            "a waste of life, all a travesty to cover the desolation of her\n",
            "unnaturally extended existence. \n",
            "\n",
            "She faced the stairs with exhaustion, deciding, instead, to forego the \n",
            "trip up. She sat on the bottom step, delicate chin propped on tightly \n",
            "curled fists, gazing dully at the open pantry door, seeing into the past \n",
            "-- again. Where, in this world the shelves were haphazardly stacked with \n",
            "cans of peaches and corn, she saw row after row of glass jars. Beets!  \n",
            "Ugh!  Her grandmother's pickled beets, always pretty to view, left a \n",
            "phantom bitterness within her mouth.\n",
            "\n",
            "On the lawn Kate sat back on her heels, suddenly lost in sorrow and\n",
            "self-pity. Tears streamed down her cheeks to drop onto the fabric of\n",
            "her dress. She thought of Harold, busily showing homes as lovely as\n",
            "their own to strangers while she ruined her nails weeding this pitiful\n",
            "excuse for a garden. She shoved her pudgy fists into her burning eyes\n",
            "and wept aloud for the waste of her life. She sniffed back her running \n",
            "nose . . . sniffed again. She snuffled like a dog scenting something \n",
            "unusual, nose in the air. \"Beets?\"  she asked aloud. \"Beets?\"  Her \n",
            "hands dropped to her thighs, pushing to rise. _\"Of course,\"_ she thought \n",
            "to herself, _\"this *lovely* house is haunted by a very emotional woman.\"_  \n",
            "Her knees ached. She turned toward the house and noticed the flour \n",
            "canisters on the bench. \"And whatever she wants *this* time is not \n",
            "getting through this thick skull of mine!\"\n",
            "\n",
            "Kate knuckle-rapped herself above her right temple. \"Rebeccah!\"  she\n",
            "called. \"Quit moping!  You'll ruin another day for me and I still\n",
            "have to deal with that horrible Avon woman this morning.\"\n",
            "\n",
            "\"I want my flowers properly aligned!\" Rebeccah screamed from the stairs.\n",
            "\n",
            "As Kate passed the bench she paused to move the flour canisters so\n",
            "that the labels faced in the same direction, each perfectly centered\n",
            "over three of the wood slats. With a self-satisfied air she re-entered \n",
            "her own kitchen. \"Now,\" she began, addressing the refrigerator, \"what \n",
            "we need is improved communication.\"\n",
            "\n",
            "\"Fool,\" hissed Rebeccah, \"you're talking to the refrigerator again.\"\n",
            "\n",
            "\"You don't want an empath. You want a telepath,\" Kate said, turning\n",
            "to stare at Rebeccah with surprising accuracy.\n",
            "\n",
            "The two women blinked at each other and broke into laughter.\n",
            "\n",
            "\"I want my flowers straightened out!\"  Rebeccah commented softly when\n",
            "the mirth had passed.\n",
            "\n",
            "                              * * *\n",
            "\n",
            "\"There!\"  Kate replaced the telephone hand piece and pocketed the\n",
            "scrap of paper she'd written the new gardener's name upon. \"Mr.\n",
            "Hi-a-cow-wah,\" she practiced aloud. \"Very good.\"  The door chime rang\n",
            "throughout the house, echoing off the tiled kitchen walls.\n",
            "\n",
            "\"Oh, no!\"  wailed Rebeccah. \"Not Japanese!  They have such spiritual\n",
            "ideas on gardening -- I'll never get through to him!\"\n",
            "\n",
            "\"Oh, dear!\"  Kate bemoaned, certain the Avon woman had come to call.\n",
            "She brushed her hands over her skirt, straightened her broad shoulders\n",
            "and pushed through to the dining room, determined not to buy a single\n",
            "thing today.\n",
            "\n",
            "\"Good morning, Mrs. Blanchard!\"  beamed the woman in the pale rose\n",
            "colored ensemble. Purse clutched in one hand, sample case in the other, \n",
            "she reminded Kate of the Lady Justice, scales perfectly balanced. But \n",
            "this lady had no blindfold. (All the better to see you with, my dear. \n",
            "And Oh, wouldn't this color just bring on the blush in your cheeks for \n",
            "$11.00 a tube?)  \"Isn't it just a glorious day?\" the woman pronouned, \n",
            "boldly stepping over the threshold on past assumptions.\n",
            "\n",
            "_\"That's it!\"_ Kate thought to herself. She'd let the woman in once,\n",
            "bought gifts soaps and lipstick in the spirit of cooperation, and\n",
            "never been free of past assumptions since. \"Glorious!\" Kate echoed,\n",
            "moving aside before she was trod upon. Rebeccah hovered at the dining\n",
            "room doors. Kate felt her there.\n",
            "\n",
            "\"Oh, and you've brought the day in with you!\" exclaimed the woman,\n",
            "noting cut flowers on mantel and coffee table. \"How healthful!\"\n",
            "\n",
            "\"Healthful?\" Kate inquired.\n",
            "\n",
            "\"Oh, yes. Studies have shown that people who surround themselves with\n",
            "live plants and fresh flowers indoors live longer, feel better, and\n",
            "enjoy life more fully.\"\n",
            "\n",
            "\"Coffee?\"  Kate offered as the woman sat on the edge of the sofa. It\n",
            "was the one torment she allowed herself to use on the woman, knowing\n",
            "full well this door to door saleswoman would shun other people's\n",
            "bathrooms.\n",
            "\n",
            "\"No thank you,\" she answered, a slight grimace flashing across her\n",
            "face as she scooted forward and opened her case.\n",
            "\n",
            "\"You're so rude!\" Rebeccah crowed, having come closer. \"She's got a\n",
            "bladder full now.\"\n",
            "\n",
            "Kate smiled, holding back a giggle. She was certain she'd scored\n",
            "without knowing why. The woman drew forth brightly colored sheets of\n",
            "paper and placed them neatly before Kate on the glass topped table.\n",
            "_\"A promotional,\"_ Kate moaned within her mind. At the bottom of each\n",
            "was stamped, in flowing script, \"Eleanor Thomsason.\"  Address and two\n",
            "phone numbers followed in block lettering.\n",
            "\n",
            "\"I don't really need anything today, Eleanor,\" Kate began.\n",
            "\n",
            "\"Of course you don't, dear. You're more than lovely in your house\n",
            "frock and clean scrubbed face. But you must see the new complexion\n",
            "care line we're offering. Designed especially for the woman over 30\n",
            "and her special needs,\" Eleanor pulled full sized display item from\n",
            "the depths of her bottomless case and set them neatly in a row,\n",
            "labels facing the prospective buyer. \"As you can see here,\" she said\n",
            "crisply, long manicured finger nail tapping each item gently as she\n",
            "spoke, \"We have a scrub, toner, tightener, moisturizer and light\n",
            "foundation. The foundation comes in 6 basic colors. Just to smooth\n",
            "over those tiny blotches we all seem to have after 30.\"\n",
            "\n",
            "Kate sat forward in her occasional chair, considering the possibility\n",
            "that she might, indeed, need a little more complexion care. She\n",
            "touched the toner, tilting it slightly to the light. While she was\n",
            "otherwise engaged Eleanor brought forth tubes, bottles and jars of the\n",
            "same line. She busied herself arranging them in a straight line to\n",
            "the left and just behind the first row.\n",
            "\n",
            "\"And here we have the corresponding blush, highlighters, lipsticks \n",
            "and shadows. Now this line is made with completely natural base\n",
            "substances,\" Eleanor pointed out.\n",
            "\n",
            "\"Chemicals,\" Rebeccah commented, coming closer still, intently\n",
            "interested in the ordered presentation.\n",
            "\n",
            "Kate let go the toner and reached for the blush. Eleanor straightened\n",
            "the toner, turning the label toward the prospective buyer. Rebeccah\n",
            "came around the coffee table and sat on the sofa with Eleanor, her\n",
            "arms primly at her sides, hands clasped in her lap. Rebeccah leaned\n",
            "forward in the same manner as did Eleanor.\n",
            "\n",
            "The genial rise and fall of the woman's voice slipped into the background \n",
            "of sounds passing by on the peaceful street outside. Kate blinked once, \n",
            "the blush still clasped within her fingers, watching Eleanor's lips move. \n",
            "She could almost hear Rebeccah.\n",
            "\n",
            "Rebeccah's attention was focused entirely on Eleanor the Avon lady.\n",
            "\"The flowers have been scattered willy-nilly along the walk,\"\n",
            "Rebeccah said conversationally, her lips mere inches from Eleanor's\n",
            "ear. \"They look so untidy.\"  Eleanor looked, suddenly, as if she'd\n",
            "forgotten something. Kate remembered the flour canisters on the\n",
            "bench. \"What we need is someone with some organizational ability,\"\n",
            "Rebeccah continued. \n",
            "\n",
            "Eleanor drew forth her order book. \"Flowers are like life's little \n",
            "markers,\" Rebeccah whispered. Eleanor reached into her case for a \n",
            "marker. \"Yellow markers, as it were, for the days of our lives.\"  \n",
            "Eleanor replaced the fine tipped black marker and retrieved a broad \n",
            "stroke yellow highlighter. Kate seemed to hear McDonald Carey speaking \n",
            "about sand. \"The flowers along the walk NEED straightening.\"\n",
            "\n",
            "\"Will you excuse me, just one moment?\" Kate asked. She knew exactly\n",
            "where to find that hourglass. She rose from her chair\n",
            "\n",
            "\"Certainly, dear,\" Eleanor answered, her mind seemingly elsewhere\n",
            "while her hands compulsively aligned the display items.\n",
            "\n",
            "\"*YOU* could be the only one for the job!\"  Rebeccah spoke\n",
            "authoritatively, her body turned toward Eleanor. \"The flowers need\n",
            "alignment!\"\n",
            "\n",
            "Kate felt an oppressive headache coming on. Two of them in one\n",
            "morning was more than anyone should be expected to bear. As she\n",
            "passed through the kitchen door her spirits seemed to rise suddenly.\n",
            "Sunshine slanted into the room to highlight every gleaming surface,\n",
            "glinting sweetly on glassware and chrome. She inhaled fully, filling\n",
            "her lungs with the aroma of fresh brewed coffee. The hourglass\n",
            "spilling out the days of her life seemed important only in the\n",
            "abstract. All was right today. She thought of the flowers by the\n",
            "walk, then. For some reason she  wanted to see them from the top\n",
            "floor.\n",
            "\n",
            "She poured herself a cup of coffee, carried it up the back stairs to\n",
            "the second floor landing and peered from the window into the side\n",
            "yard. She thought, idly, of the new gardener, and what creative\n",
            "expression he might come up with for that spot there, which had never\n",
            "been cultivated. Onward, to the front of the house, and into the\n",
            "quiet room beneath the pitch of the front eaves. \n",
            "\n",
            "She sat on the window ledge and balanced her cup on the sill, the \n",
            "threatened headache a memory, only, of Saturday afternoons with her \n",
            "mother. Somewhere behind her temples her mother's voice droned on and \n",
            "on; something about book spines and the edge of the shelf. Sometimes \n",
            "one had to learn to ignore the librarian in order to read the books.\n",
            "\n",
            "Her eyes drifted to the front walk. Far below, as if in another\n",
            "world, Eleanor the Avon lady knelt in the grass next to the walk.\n",
            "A tall shadow stood near, softly, insistently coaxing, as Eleanor\n",
            "carefully spaded deep into the earth and removed a daffodil. She\n",
            "placed it gently into a prepared hole, tamped the earth around it and\n",
            "proceeded to dig another hole, exactly six inches from the last, in a\n",
            "perfectly straight line parallel to the walk.\n",
            "\n",
            "\"Oh, for crying out loud!\"  Kate exclaimed, watching closely. \"Those\n",
            "flowers!\"  She'd have to remember to collect the flour canisters\n",
            "before Harold came home. \"Goodness, Rebeccah,\" she continued, with\n",
            "some exasperation, \"why on earth didn't you say `Daffodils'?\"\n",
            "\n",
            "                              # # #\n",
            "\n",
            "Copyright 1994 Gay Bost\n",
            "--------------------------------------------------------------------------\n",
            " Gay is a Clinical Lab Tech with experience in Veterinary medicine. \n",
            "Originally from NORTHERN California, she has resided in Southeast Missouri \n",
            "with her husband and an aggressive 6 year old boy, since 1974. She \n",
            "installed her first modem in the summer of 1992 and has been exploring new \n",
            "worlds since. Her first and only publication, a short horror story, came \n",
            "when she was 17 years old. The success was so overwhelming she called an \n",
            "end to her writing days and went in search of herself. She's still looking. \n",
            "You will find Gay's work in the best Electronic Magazines.\n",
            "===========================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWIBRndGXaUz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21k_zyVzXaU0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}